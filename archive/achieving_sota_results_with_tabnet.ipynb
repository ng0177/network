{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "achieving-sota-results-with-tabnet.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ng0177/network/blob/main/achieving_sota_results_with_tabnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About this Notebook\n",
        "If anyone of you have read my previous kernels , you might know how much I love the EDA part , but it struck me that writing on one particular thing would not help me grow , so I have decided to explore untreaded territories to explore new things. For this competition people are mostly using Rapids and the tabular data . I have hardly seen any kernels using only images and both images and tabular data .\n",
        "\n",
        "Few days ago I saw Abhishek's post on LinkedIn about Tabnet and I was really curious about it , I wanted to apply the idea here on Trends data but it had already been done and didn't give good results so I dropped it.\n",
        "\n",
        "After watching Sebastian on Abhishek talks , I realized that Tabnet's potential isn't being fully utilized .\n",
        "\n",
        "**This notebook presents a fully structured working pipeline for training n-folds Tabnet Regressor for this competition . This Notebook achieves 0.1620 without a lot of efforts and this notbook could beat Rapids SVM's and achieve the benchmark 0.1595 with some tweaks . I also explain the pros and cons of using Tabnets (although I don't find a lot cons ðŸ˜œ )**\n",
        "\n",
        "Here is the [link](https://arxiv.org/pdf/1908.07442.pdf) to Tabnet Paper\n",
        "\n",
        "<font color='red'>If you like my efforts please leava an upvote .As I am not planning on doing this competition for now , if you all like my efforts I plan to release more public kernels on Tabnet with higher scores</font>"
      ],
      "metadata": {
        "id": "3pIZ1jIIir12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token of Gratitude\n",
        "\n",
        "* For the part other than modelling I have used most of the code from this wonderful [kernel](https://www.kaggle.com/aerdem4/rapids-svm-on-trends-neuroimaging) by Ahmet , Thank you for writing it \n",
        "* A big thanks to team of Pytorch-Tabnet for writing such a beautiful implementations with so much functionalities . The repo can be found [here](https://github.com/dreamquark-ai/tabnet)\n",
        "The documentation is very nicely written and Sebastien has also provided with example notebooks to help understand the model and usage better. Everything can be found at above mentioned repo"
      ],
      "metadata": {
        "id": "yaftRS8vir1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advantages of Tabnet\n",
        "\n",
        "Tabnet gives us the following advantages :-\n",
        "* The best thing which I found is Tabnet allows us to train a MULTIREGRESSOR and we don't to create separate models for every class\n",
        "\n",
        "* It uses attention for selecting out the set of features to focus on for a given particular data point and we can even visualize that to see which parts get attention for a particular decision . We can also play with the number of features we want the Tabnet to focus to.\n",
        "* It uses backprop for improving decisions and weights thus providing a greater control to us\n",
        "* We can use the fine-tuning techniques that have worked for us and all the deep-learning concepts like LR annealing , Custom loss,etc\n",
        "* The headache of feature selection is vanished as Tabnet does that on its own.\n",
        "* It achieves SOTA results wothout any feature engg, finetuning with just  the defaults , wonder what it can do with sufficient feature engineering and finetuning\n",
        "\n",
        "There are a lot of more advantages and ideas that I have for Tabnet which I plan to release in the future\n",
        "\n",
        "If you want to learn more about Tabnet and it's inner workings please refer to this [video](https://www.youtube.com/watch?v=ysBaZO8YmX8)"
      ],
      "metadata": {
        "id": "3LqoOG6uir2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-tabnet"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQRMGMMRir2B",
        "outputId": "5c96d2ac-ecc7-4aea-9eff-437888d6fb75"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-tabnet\n",
            "  Downloading pytorch_tabnet-3.1.1-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.36 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (4.64.0)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.0.2)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.7.3)\n",
            "Requirement already satisfied: torch<2.0,>=1.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.12.1+cu113)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2.0,>=1.2->pytorch-tabnet) (4.1.1)\n",
            "Installing collected packages: pytorch-tabnet\n",
            "Successfully installed pytorch-tabnet-3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preliminaries\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import os\n",
        "import random\n",
        "\n",
        "#Visuals\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#Torch and Tabnet\n",
        "import torch\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "\n",
        "#Sklearn only for splitting\n",
        "from sklearn.model_selection import KFold"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "67oKVcNQir2D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "czlVOu_mir2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_FOLDS = 7  # you can specify your folds here\n",
        "seed = 2020   # seed for reproducible results"
      ],
      "metadata": {
        "trusted": true,
        "id": "KySVqiIJir2G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seed Everything\n",
        "\n",
        "Seeding Everything for Reproducible Results"
      ],
      "metadata": {
        "id": "5zmTKWXLir2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "trusted": true,
        "id": "J-WN5PXlir2I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(seed)"
      ],
      "metadata": {
        "trusted": true,
        "id": "nlZxW528ir2J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metric\n",
        "\n",
        "Since Tabnet allows us to create a MULTIREGRESSOR , we don't have to create multiple models and loop through them . I have modified the metric to account for that"
      ],
      "metadata": {
        "id": "RRahDbvxir2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metric(y_true, y_pred):\n",
        "    \n",
        "    overall_score = 0\n",
        "    \n",
        "    weights = [.3, .175, .175, .175, .175]\n",
        "    \n",
        "    for i,w in zip(range(y_true.shape[1]),weights):\n",
        "        ind_score = np.mean(np.sum(np.abs(y_true[:,i] - y_pred[:,i]), axis=0)/np.sum(y_true[:,i], axis=0))\n",
        "        overall_score += w*ind_score\n",
        "    \n",
        "    return overall_score"
      ],
      "metadata": {
        "trusted": true,
        "id": "7Xuag2jsir2K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation\n",
        "\n",
        "Mostly Taken from Ahmet's kernel"
      ],
      "metadata": {
        "id": "_YDKZhT9ir2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnc_df = pd.read_csv(\"../input/trends-assessment-prediction/fnc.csv\")\n",
        "loading_df = pd.read_csv(\"../input/trends-assessment-prediction/loading.csv\")\n",
        "\n",
        "fnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\n",
        "df = fnc_df.merge(loading_df, on=\"Id\")\n",
        "features = fnc_features + loading_features\n",
        "\n",
        "\n",
        "labels_df = pd.read_csv(\"../input/trends-assessment-prediction/train_scores.csv\")\n",
        "target_features = list(labels_df.columns[1:])\n",
        "labels_df[\"is_train\"] = True\n",
        "\n",
        "\n",
        "df = df.merge(labels_df, on=\"Id\", how=\"left\")\n",
        "\n",
        "test_df = df[df[\"is_train\"] != True].copy()\n",
        "df = df[df[\"is_train\"] == True].copy()\n",
        "\n",
        "df.shape, test_df.shape"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "SXJ5jeZnir2L",
        "outputId": "d9057f06-75a2-40a2-ec36-1f8222c4bdd3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-014969f157dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfnc_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/trends-assessment-prediction/fnc.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloading_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/trends-assessment-prediction/loading.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfnc_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloading_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfnc_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloading_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfnc_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloading_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/trends-assessment-prediction/fnc.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating FOLDS\n",
        "\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "df[\"kfold\"] = -1\n",
        "\n",
        "df = df.sample(frac=1,random_state=2020).reset_index(drop=True)\n",
        "\n",
        "kf = KFold(n_splits=NUM_FOLDS)\n",
        "\n",
        "for fold, (trn_, val_) in enumerate(kf.split(X=df, y=df)):\n",
        "    df.loc[val_, 'kfold'] = fold"
      ],
      "metadata": {
        "trusted": true,
        "id": "-3kjzbZjir2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Giving less importance to FNC features since they are easier to overfit due to high dimensionality.\n",
        "FNC_SCALE = 1/500\n",
        "\n",
        "df[fnc_features] *= FNC_SCALE\n",
        "test_df[fnc_features] *= FNC_SCALE"
      ],
      "metadata": {
        "trusted": true,
        "id": "rW3r_lnjir2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "NSECALu_ir2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TabNetRegressor(n_d=16,\n",
        "                       n_a=16,\n",
        "                       n_steps=4,\n",
        "                       gamma=1.9,\n",
        "                       n_independent=4,\n",
        "                       n_shared=5,\n",
        "                       seed=seed,\n",
        "                       optimizer_fn = torch.optim.Adam,\n",
        "                       scheduler_params = {\"milestones\": [150,250,300,350,400,450],'gamma':0.2},\n",
        "                       scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)"
      ],
      "metadata": {
        "trusted": true,
        "id": "0Brs4YZXir2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Engine"
      ],
      "metadata": {
        "id": "vksSD6nFir2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = np.zeros((test_df.shape[0],len(target_features), NUM_FOLDS))  #A 3D TENSOR FOR STORING RESULTS OF ALL FOLDS"
      ],
      "metadata": {
        "trusted": true,
        "id": "DEDgBYXHir2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(fold):\n",
        "    df_train = df[df.kfold != fold]\n",
        "    df_valid = df[df.kfold == fold]\n",
        "    \n",
        "    X_train = df_train[features].values\n",
        "    Y_train = df_train[target_features].values\n",
        "    \n",
        "    X_valid = df_valid[features].values\n",
        "    Y_valid = df_valid[target_features].values\n",
        "    \n",
        "    y_oof = np.zeros((df_valid.shape[0],len(target_features)))   # Out of folds validation\n",
        "    \n",
        "    print(\"--------Training Begining for fold {}-------------\".format(fold+1))\n",
        "     \n",
        "    model.fit(X_train = X_train,\n",
        "             y_train = Y_train,\n",
        "             X_valid = X_valid,\n",
        "             y_valid = Y_valid,\n",
        "             max_epochs = 1000,\n",
        "             patience =70)\n",
        "              \n",
        "    \n",
        "    print(\"--------Validating For fold {}------------\".format(fold+1))\n",
        "    \n",
        "    y_oof = model.predict(X_valid)\n",
        "    y_test[:,:,fold] = model.predict(test_df[features].values)\n",
        "    \n",
        "    val_score = metric(Y_valid,y_oof)\n",
        "    \n",
        "    print(\"Validation score: {:<8.5f}\".format(val_score))\n",
        "    \n",
        "    # VISUALIZTION\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.plot(model.history['train']['loss'])\n",
        "    plt.plot(model.history['valid']['loss'])\n",
        "    \n",
        "    #Plotting Metric\n",
        "    #plt.plot([-x for x in model.history['train']['metric']])\n",
        "    #plt.plot([-x for x in model.history['valid']['metric']])"
      ],
      "metadata": {
        "trusted": true,
        "id": "u4Xxljmiir2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I am hiding the output of training please unhide the output to look at the results and Loss plots for any fold"
      ],
      "metadata": {
        "id": "AHrwSbjWir2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run(fold=0)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "u3YdOaO1ir2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run(fold=1)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "FPYZ-Kexir2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run(fold=2)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "YcBrlDK4ir2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run(fold=3)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "ipAkqcCxir2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run(fold=4)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "nwDjhjupir2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run(fold=5)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "KhcDnGbAir2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run(fold=6)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "-17F8XLQir2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Submission"
      ],
      "metadata": {
        "id": "PorFrsoWir2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = y_test.mean(axis=-1) # Taking mean of all the fold predictions\n",
        "test_df[target_features] = y_test"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZnXoajlgir2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]]"
      ],
      "metadata": {
        "trusted": true,
        "id": "2X1bY9SVir2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_df = pd.melt(test_df, id_vars=[\"Id\"], value_name=\"Predicted\")\n",
        "sub_df[\"Id\"] = sub_df[\"Id\"].astype(\"str\") + \"_\" +  sub_df[\"variable\"].astype(\"str\")\n",
        "\n",
        "sub_df = sub_df.drop(\"variable\", axis=1).sort_values(\"Id\")\n",
        "assert sub_df.shape[0] == test_df.shape[0]*5\n",
        "sub_df.head(10)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ETGh645wir2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_df.to_csv('submission.csv',index=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "t0dVWkTgir2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End Notes:\n",
        "* Tabnet allows us to have a greater control over training and predictions\n",
        "* With Tabnet we can integrate Image and Tabular data with some ideas\n",
        "* I have dropped the missing values in the targets and used raw data without any pre-processing/feature engineering ,etc\n",
        "* I would be glad to see interesting results if someone fine tunes  it further"
      ],
      "metadata": {
        "id": "f_WhT7TXir2Q"
      }
    }
  ]
}